---
layout: post
title: Flax/JAXë¡œ ì‹œì‘í•˜ëŠ” ë”¥ëŸ¬ë‹
categories: deep_learning
tags: [Deep Learing, Flax, JAX]
toc: true
comments: true
excerpt_separator: <!--more-->
---

> ~~_"í”Œë ‰ìŠ¤ í•´ë²„ë ¸ì§€ ë­ì•¼"_~~

## ê°œìš”

[JAX](https://github.com/google/jax)ê°€ ì¶œì‹œëœì§€ 2ë…„ì´ ì§€ë‚˜ë©´ì„œ ì™„ì„±ë„ê°€ ë§ì´ ë†’ì•„ì¡ŒìŠµë‹ˆë‹¤. ì´ì œëŠ” ì‹¤í—˜ ë‹¨ê³„ë¥¼ ë„˜ì–´ ìƒìš© ëª¨ë¸ ê°œë°œì— JAX ë„ì…ì„ ê²€í† í•´ ë³¼ë§Œí•œ ë‹¨ê³„ê°€ ì˜¨ ê²ƒ ê°™ìŠµë‹ˆë‹¤. DeepMindë‚˜ Hugging Faceì™€ ê°™ì€ AI ì—…ê³„ë¥¼ ì„ ë„í•˜ëŠ” íšŒì‚¬ë“¤ì€ ì´ë¯¸ JAXë¡œ ì´ì „í–ˆê±°ë‚˜ ì´ì „í•˜ê³  ìˆëŠ” ê³¼ì •ì— ìˆê¸°ë„ í•˜êµ¬ìš”. ([ì°¸ê³ ë§í¬1](https://www.deepmind.com/blog/using-jax-to-accelerate-our-research), [ì°¸ê³ ë§í¬2](https://twitter.com/huggingface/status/1331255460033400834))

ì´ ê¸°ì‚¬ì—ëŠ” [Flax](https://github.com/google/flax)ë¼ëŠ” JAXìš© ë”¥ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬, ê°„ë‹¨í•œ ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ë§Œë“œëŠ” ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤. Flaxì˜ APIëŠ” ë”¥ëŸ¬ë‹ ê°œë°œì ê²½í—˜ (DX, Developer Experience) ê³ ë ¤í•˜ì—¬ ê°œë°œ ë˜ì—ˆê¸°ì— ê¸°ì¡´ Tensorflowë‚˜ PyTorch ê²½í—˜ì´ ìˆëŠ” ë¶„ì´ë©´ ì‰½ê²Œ ìµí ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<!--more-->

### JAX

[JAX](https://github.com/google/jax)ëŠ” ìë™ ë¯¸ë¶„ ê¸°ëŠ¥ (AutoGrad)ë¥¼ ê°€ì§€ê³  ìˆëŠ” CPU/GPU/TPUì—ì„œ ë™ì‘í•˜ëŠ” NumPy ì…ë‹ˆë‹¤.

ê¸°ì¡´ NumPy API(+í•„ìš”ì‹œ íŠ¹ìˆ˜í•œ ê°€ì†ê¸° ì‘ì—…ì„ ìœ„í•œ ì¶”ê°€ì ì¸ API)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹ ë¥¸ ê³¼í•™ì  ê³„ì‚° ë° ë¨¸ì‹  ëŸ¬ë‹ì„ ì œê³µí•©ë‹ˆë‹¤.

JAXëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê°•ë ¥í•œ ê¸°ë³¸ ìš”ì†Œë“¤ì´ ì œê³µë©ë‹ˆë‹¤:

- **Autodiff (`jax.grad`)**: ëª¨ë“  ë³€ìˆ˜ì— ëŒ€í•œ íš¨ìœ¨ì ì¸ ì„ì˜ì˜ ì°¨ìˆ˜ ê·¸ë ˆì´ë””ì–¸íŠ¸
- **[JIT (Just-In-Time) ì»´íŒŒì¼](https://ko.wikipedia.org/wiki/JIT_%EC%BB%B4%ED%8C%8C%EC%9D%BC) (`jax.jit`)**: ëª¨ë“  ê¸°ëŠ¥ ì¶”ì  â†’ í“¨ì „ëœ ê°€ì†ê¸° ops
- **ë²¡í„°í™” (`jax.vmap`)**: ê°œë³„ ìƒ˜í”Œì— ëŒ€í•œ ìë™ ë°°ì¹˜ ì½”ë“œ ì‘ì„±
- **ë³‘ë ¬í™” (`jax.pmap`)**: ì—¬ëŸ¬ ê°€ì†ê¸°(ì˜ˆ: TPU í¬ë“œë¥¼ ìœ„í•œ í˜¸ìŠ¤íŠ¸ë“¤ í¬í•¨) ê°„ì— ìë™ìœ¼ë¡œ ì½”ë“œ ë³‘ë ¬í™”

### Flax

[Flax](https://github.com/google/flax)ëŠ” ìœ ì—°ì„±ì„ ìœ„í•´ ì„¤ê³„ëœ JAXë¥¼ ìœ„í•œ ë”¥ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì…ë‹ˆë‹¤. FlaxëŠ” í”„ë ˆì„ì›Œí¬ì— ê¸°ëŠ¥ì„ ì¶”ê°€í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì˜ˆì œì™€ í•™ìŠµ ë£¨í”„ë¥¼ ìˆ˜ì •í•˜ì—¬ ìƒˆë¡œìš´ í˜•íƒœì˜ ëª¨ë¸ í•™ìŠµì„ ì‹œë„í•©ë‹ˆë‹¤.

FlaxëŠ” JAX íŒ€ê³¼ ê¸´ë°€íˆ í˜‘ë ¥í•˜ì—¬ ê°œë°œ ì¤‘ì´ë©° ë‹¤ìŒê³¼ ê°™ì€ ë”¥ëŸ¬ë‹ ì—°êµ¬ë¥¼ ì‹œì‘í•˜ëŠ” ë° í•„ìš”í•œ ëª¨ë“  ê²ƒì´ ì œê³µë©ë‹ˆë‹¤:

- **ì‹ ê²½ë§ API (`flax.linen`)**: Dense, Conv, Batch/Layer/Group ì •ê·œí™”, Attention, Pooling, LSTM/GRU ì…€, Dropout
- **ìœ í‹¸ë¦¬í‹° ë° íŒ¨í„´**: ë³µì œëœ í•™ìŠµ, ì§ë ¬í™” ë° ì²´í¬í¬ì¸íŠ¸, ë©”íŠ¸ë¦­, ì¥ì¹˜ì—ì„œ ì‚¬ì „ ê²€ìƒ‰
- ì¦‰ì‹œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” **í•™ìŠµ ì˜ˆì œ**: MNIST, LSTM seq2seq, GNN (Graph Neural Networks), ì‹œí€€ìŠ¤ íƒœê¹…
- **ë¹ ë¥´ê³  íŠœë‹ëœ ëŒ€ê·œëª¨ ì¢…ë‹¨ê°„ ì˜ˆì œ**: CIFAR10, ImageNetì˜ ResNet, Transformer LM1b

### Flaxë¥¼ ë°°ì›Œì•¼ í•˜ëŠ” ì´ìœ 

> 2022-7-13ì¼ì— ì¶”ê°€í•œ ë‚´ìš© ì…ë‹ˆë‹¤.

ì´ë¯¸ PyTorchë‚˜ Tensorflowë¡œ ë”¥ëŸ¬ë‹ ì—°êµ¬ & ê°œë°œì„ ì˜í•˜ê³  ìˆëŠ”ë° ì™œ Flaxë¥¼ ë°°ì›Œì•¼ í• ê¹Œìš”?

- Flaxì˜ ëª¨ë¸ ì •ì˜ëŠ” ê¸°ì¡´ ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›ê³¼ ê±°ì˜ ì°¨ì´ê°€ ì—†ìŠµë‹ˆë‹¤.
- FlaxëŠ” ì„¤ê³„ìƒ ë§¤ìš° ìœ ì—°í•˜ê³  í™•ì¥ ê°€ëŠ¥í•©ë‹ˆë‹¤.
- **ì½”ë“œ ë³€ê²½ ì—†ì´ [TPU](https://cloud.google.com/tpu?hl=ko) í•™ìŠµì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.**
- GPU í•™ìŠµì‹œ ì…ë ¥ ë°ì´í„°ê°€ í¬ë©´ PyTorchì— ë¹„í•´ í•™ìŠµ ì‹œê°„ì´ ë¹ ë¦…ë‹ˆë‹¤. ([ì°¸ê³ ë§í¬](https://github.com/google/jax/discussions/8497#discussioncomment-1626017))

í˜„ì¬ê¹Œì§€ëŠ” Flaxì—ëŠ” ë°ì´í„° ë¡œë“œ ë° ì²˜ë¦¬ ê¸°ëŠ¥ì´ ì—†ì–´ì„œ PyTorchì˜ [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)ë‚˜ [transpose](https://pytorch.org/docs/stable/generated/torch.transpose.html) ê°™ì€ ê²ƒì„ JAXì™€ ì¡°í•©í•˜ì—¬ ì§ì ‘ êµ¬í˜„í•´ì•¼í•˜ëŠ” ë¶ˆí¸í•¨ì´ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ, ìœ„ì™€ ê°™ì€ ì¥ì ë“¤ì´ ìˆìœ¼ë¯€ë¡œ ì‹œê°„ì„ ë“¤ì—¬ í•œë²ˆì¯¤ í•™ìŠµì„ í•˜ëŠ”ê±´ ë‚˜ì˜ì§€ ì•Šë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤. íŠ¹íˆ, ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•´ Transformer ê³„ì—´ ëª¨ë¸ì„ ì‹¤í—˜ í•˜ì‹œëŠ” ë¶„ê»˜ ì¶”ì²œ ë“œë¦½ë‹ˆë‹¤.

- Flax Transformer ì˜ˆì œ: [Fine-tuning a ğŸ¤— Transformers model on TPU with Flax/JAX](https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/text_classification_flax.ipynb)

## ì˜ë¥˜ ì´ë¯¸ì§€ ë¶„ë¥˜ ì˜ˆì œ

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1kUtM8o62QPP0BkzXHPmHmXgsdPOmBynd?usp)

ì´ ì¥ì—ì„œëŠ” Tensorflow ê³µì‹ ë¬¸ì„œì— ìˆëŠ” [ê¸°ë³¸ ë¶„ë¥˜: ì˜ë¥˜ ì´ë¯¸ì§€ ë¶„ë¥˜](https://www.tensorflow.org/tutorials/keras/classification?hl=ko) ì˜ˆì œë¥¼ Flaxë¡œ êµ¬í˜„í•´ë³´ê² ìŠµë‹ˆë‹¤.

ë¨¼ì €, Flax íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ê³  í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ import í•©ë‹ˆë‹¤.

```
!pip install -q flax
```

```python
import jax
import jax.numpy as jnp                # JAX NumPy
import numpy as np                     # ê¸°ì¡´ NumPy

from flax import linen as nn           # The Linen API
from flax.training import train_state  # í•™ìŠµ ìƒíƒœ ë³´ì¡´ì— ìœ ìš©í•œ ë°ì´í„° í´ë˜ìŠ¤

import optax                           # Optimizers
import tensorflow_datasets as tfds     # Fashion MNIST Dataset ê°€ì ¸ì˜¤ê¸° ìœ„í•œ TFDS íŒ¨í‚¤ì§€
from matplotlib import pyplot as plt   # ì‹œê°í™”
```

### ë°ì´í„°ì…‹ ê°€ì ¸ì˜¤ê¸°

10ê°œì˜ ë²”ì£¼(category)ì™€ 70,000ê°œì˜ í‘ë°± ì´ë¯¸ì§€ë¡œ êµ¬ì„±ëœ [íŒ¨ì…˜ MNIST ë°ì´í„°ì…‹](https://github.com/zalandoresearch/fashion-mnist)ì„ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ëŠ” í•´ìƒë„(28x28 í”½ì…€)ê°€ ë‚®ê³  ë‹¤ìŒì²˜ëŸ¼ ê°œë³„ ì˜· í’ˆëª©ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤:

![Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist/raw/master/doc/img/fashion-mnist-sprite.png)

- ê·¸ë¦¼ 1. íŒ¨ì…˜-MNIST ìƒ˜í”Œ

íŒ¨ì…˜ MNISTëŠ” ì»´í“¨í„° ë¹„ì „ ë¶„ì•¼ì˜ "Hello, World" í”„ë¡œê·¸ë¨ê²©ì¸ ê³ ì „ MNIST ë°ì´í„°ì…‹ì„ ëŒ€ì‹ í•´ì„œ ìì£¼ ì‚¬ìš©ë©ë‹ˆë‹¤. MNIST ë°ì´í„°ì…‹ì€ ì†ê¸€ì”¨ ìˆ«ì(0, 1, 2 ë“±)ì˜ ì´ë¯¸ì§€ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ ì‚¬ìš©í•˜ë ¤ëŠ” ì˜· ì´ë¯¸ì§€ì™€ ë™ì¼í•œ í¬ë§·ì…ë‹ˆë‹¤.

íŒ¨ì…˜ MNISTëŠ” ì¼ë°˜ì ì¸ MNIST ë³´ë‹¤ ì¡°ê¸ˆ ë” ì–´ë ¤ìš´ ë¬¸ì œì´ê³  ë‹¤ì–‘í•œ ì˜ˆì œë¥¼ ë§Œë“¤ê¸° ìœ„í•´ ì„ íƒí–ˆìŠµë‹ˆë‹¤. ë‘ ë°ì´í„°ì…‹ì€ ë¹„êµì  ì‘ê¸° ë•Œë¬¸ì— ì•Œê³ ë¦¬ì¦˜ì˜ ì‘ë™ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë˜ê³¤ í•©ë‹ˆë‹¤. ì½”ë“œë¥¼ í…ŒìŠ¤íŠ¸í•˜ê³  ë””ë²„ê¹…í•˜ëŠ” ìš©ë„ë¡œ ì¢‹ìŠµë‹ˆë‹¤.

ì—¬ê¸°ì—ì„œ 60,000ê°œì˜ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ ë„¤íŠ¸ì›Œí¬ë¥¼ í›ˆë ¨í•˜ê³  10,000ê°œì˜ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ ë„¤íŠ¸ì›Œí¬ì—ì„œ ì´ë¯¸ì§€ ë¶„ë¥˜ë¥¼ í•™ìŠµí•œ ì •ë„ë¥¼ í‰ê°€í•©ë‹ˆë‹¤. Fashion MNIST ë°ì´í„°ì…‹ì€ TFDS (Tensorflow Dataset) íŒ¨í‚¤ì§€ì— í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. TFDSì—ì„œ Fashion MNIST ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ê³  ë¡œë“œí•©ë‹ˆë‹¤.

```python
ds_builder = tfds.builder('fashion_mnist')
ds_builder.download_and_prepare()
train_ds = tfds.as_numpy(ds_builder.as_dataset(split='train', batch_size=-1))
test_ds = tfds.as_numpy(ds_builder.as_dataset(split='test', batch_size=-1))
train_ds['image'] = jnp.float32(train_ds['image']) / 255.
test_ds['image'] = jnp.float32(test_ds['image']) / 255.
```

TFDS Fashion MNIST ë°ì´í„°ì„¸íŠ¸ë¥¼ `train`ê³¼ `test` ë°ì´í„°ì„¸íŠ¸ë¡œ ë¶„ë¦¬í•˜ì˜€ìŠµë‹ˆë‹¤. ê° ë°ì´í„°ì„¸íŠ¸ëŠ” `as_numpy()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ Numpy íƒ€ì…ìœ¼ë¡œ ë³€í™˜í•˜ì˜€ìŠµë‹ˆë‹¤.

ì´ë¯¸ì§€ ë°ì´í„°ëŠ” í”½ì…€ ê°’ì˜ ë²”ìœ„ê°€ 0\~255 ì‚¬ì´ì´ë¯€ë¡œ 0\~1 ì‚¬ì´ì˜ ë¶€ë™ ì†Œìˆ˜ì  íƒ€ì…ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë³€í™˜í•˜ì˜€ìŠµë‹ˆë‹¤.

<table>
  <tr>
    <th>ë ˆì´ë¸”</th>
    <th>í´ë˜ìŠ¤</th>
  </tr>
  <tr>
    <td>0</td>
    <td>T-shirt/top</td>
  </tr>
  <tr>
    <td>1</td>
    <td>Trouser</td>
  </tr>
    <tr>
    <td>2</td>
    <td>Pullover</td>
  </tr>
    <tr>
    <td>3</td>
    <td>Dress</td>
  </tr>
    <tr>
    <td>4</td>
    <td>Coat</td>
  </tr>
    <tr>
    <td>5</td>
    <td>Sandal</td>
  </tr>
    <tr>
    <td>6</td>
    <td>Shirt</td>
  </tr>
    <tr>
    <td>7</td>
    <td>Sneaker</td>
  </tr>
    <tr>
    <td>8</td>
    <td>Bag</td>
  </tr>
    <tr>
    <td>9</td>
    <td>Ankle boot</td>
  </tr>
</table>

ê° ì´ë¯¸ì§€ëŠ” í•˜ë‚˜ì˜ ë ˆì´ë¸”ì— ë§¤í•‘ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë°ì´í„°ì…‹ì— *í´ë˜ìŠ¤ ì´ë¦„*ì´ ë“¤ì–´ìˆì§€ ì•Šê¸° ë•Œë¬¸ì— ë‚˜ì¤‘ì— ì´ë¯¸ì§€ë¥¼ ì¶œë ¥í•  ë•Œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ë³„ë„ì˜ ë³€ìˆ˜ë¥¼ ë§Œë“¤ì–´ ì €ì¥í•©ë‹ˆë‹¤:

```python
class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']
```

### ë°ì´í„° íƒìƒ‰

ëª¨ë¸ì„ í›ˆë ¨í•˜ê¸° ì „ì— ë°ì´í„°ì…‹ êµ¬ì¡°ë¥¼ ì‚´í´ë³´ì£ . ë‹¤ìŒ ì½”ë“œëŠ” í›ˆë ¨ ì„¸íŠ¸ì— 60,000ê°œì˜ ì´ë¯¸ì§€ê°€ ìˆë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ê° ì´ë¯¸ì§€ëŠ” 28x28 í”½ì…€ë¡œ í‘œí˜„ë©ë‹ˆë‹¤:

```python
train_ds['image'].shape
```

> (60000, 28, 28, 1)

ë¹„ìŠ·í•˜ê²Œ í›ˆë ¨ ì„¸íŠ¸ì—ëŠ” 60,000ê°œì˜ ë ˆì´ë¸”ì´ ìˆìŠµë‹ˆë‹¤:

```python
len(train_ds['label'])
```

> 60000

ê° ë ˆì´ë¸”ì€ 0ê³¼ 9ì‚¬ì´ì˜ ì •ìˆ˜ì…ë‹ˆë‹¤:

```python
train_ds['label']
```

> array([2, 1, 8, ..., 6, 9, 9])

í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ëŠ” 10,000ê°œì˜ ì´ë¯¸ì§€ê°€ ìˆìŠµë‹ˆë‹¤. ì´ ì´ë¯¸ì§€ë„ 28x28 í”½ì…€ë¡œ í‘œí˜„ë©ë‹ˆë‹¤:

```python
test_ds['image'].shape
```

> (10000, 28, 28, 1)

í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ëŠ” 10,000ê°œì˜ ì´ë¯¸ì§€ì— ëŒ€í•œ ë ˆì´ë¸”ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤:

```python
len(test_ds['label'])
```

> 10000

í›ˆë ¨ ì„¸íŠ¸ì—ì„œ ì²˜ìŒ 25ê°œ ì´ë¯¸ì§€ì™€ ê·¸ ì•„ë˜ í´ë˜ìŠ¤ ì´ë¦„ì„ ì¶œë ¥í•´ ë³´ì£ . ë°ì´í„° í¬ë§·ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ê³  ë„¤íŠ¸ì›Œí¬ êµ¬ì„±ê³¼ í›ˆë ¨í•  ì¤€ë¹„ë¥¼ ë§ˆì¹©ë‹ˆë‹¤.

```python
plt.figure(figsize=(10,10))
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(jnp.squeeze(train_ds['image'])[i], cmap=plt.cm.binary)
    plt.xlabel(class_names[train_ds['label'][i]])
plt.show()
```

![Train data](https://github.com/Taehun/taehun.github.io/blob/main/imgs/train_result.jpg?raw=true)

### ë„¤íŠ¸ì›Œí¬ ì •ì˜

Flax Linen APIì˜ Moduleì˜ ì„œë¸Œ í´ë˜ìŠ¤ë¡œ CNN ë„¤íŠ¸ì›Œí¬ë¥¼ ì •ì˜ í•©ë‹ˆë‹¤. ì´ ì˜ˆì œì˜ ëª¨ë¸ ì•„í‚¤í…ì²˜ëŠ” ë¹„êµì  ë‹¨ìˆœí•˜ê¸° ë•Œë¬¸ì— `__call__` ë©”ì„œë“œ ë‚´ì—ì„œ ì§ì ‘ ì¸ë¼ì¸ ì„œë¸Œ ëª¨ë“ˆì„ ì •ì˜í•˜ê³  `@compact` ë°ì½”ë ˆì´í„°ë¡œ ë˜í•‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
class CNN(nn.Module):
  """A simple CNN model."""

  @nn.compact
  def __call__(self, x):
    x = nn.Conv(features=32, kernel_size=(3, 3))(x)
    x = nn.relu(x)
    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))
    x = nn.Conv(features=64, kernel_size=(3, 3))(x)
    x = nn.relu(x)
    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))
    x = x.reshape((x.shape[0], -1))  # flatten
    x = nn.Dense(features=256)(x)
    x = nn.relu(x)
    x = nn.Dense(features=10)(x)
    return x
```

### ì†ì‹¤ í•¨ìˆ˜ ì •ì˜

ê°„ë‹¨í•˜ê²Œ `optax` íŒ¨í‚¤ì§€ì˜ `softmax_cross_entropy()`ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ í•¨ìˆ˜ëŠ” _`[batch, num_classes]`_ shapeì„ ê°€ì§„ `logits`ê³¼ `labels` íŒŒë¼ë©”í„°ë¥¼ ë°›ìŠµë‹ˆë‹¤. `labels`ëŠ” TFDSì—ì„œ ì •ìˆ˜ ê°’ìœ¼ë¡œ ì½íˆë¯€ë¡œ ë¨¼ì € One-Hot ì¸ì½”ë”©ìœ¼ë¡œ ë³€í™˜í•´ì•¼ í•©ë‹ˆë‹¤.

```python
def cross_entropy_loss(*, logits, labels):
  labels_onehot = jax.nn.one_hot(labels, num_classes=10) # Ont-Hot ì¸ì½”ë”©ìœ¼ë¡œ [batch, num_classes] shapeìœ¼ë¡œ ë³€í™˜
  return optax.softmax_cross_entropy(logits=logits, labels=labels_onehot).mean()
```

### ë§¤íŠ¸ë¦­ ê³„ì‚°

ì†ì‹¤ (loss) ë° ì •í™•ë„ (accuracy) ë§¤íŠ¸ë¦­ ê³„ì‚° í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.

```python
def compute_metrics(*, logits, labels):
  loss = cross_entropy_loss(logits=logits, labels=labels)
  accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)
  metrics = {
      'loss': loss,
      'accuracy': accuracy,
  }
  return metrics
```

### í•™ìŠµ ìƒíƒœ ìƒì„±

Flaxì˜ ì¼ë°˜ì ì¸ íŒ¨í„´ì€ step ë²ˆí˜¸, íŒŒë¼ë©”í„° ë° ì˜µí‹°ë§ˆì´ì € ìƒíƒœë¥¼ í¬í•¨í•˜ì—¬ ì „ì²´ í•™ìŠµ ìƒíƒœë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë‹¨ì¼ ë°ì´í„° í´ë˜ìŠ¤ë¥¼ ë§Œë“œëŠ” ê²ƒì…ë‹ˆë‹¤.

ë˜í•œ ì˜µí‹°ë§ˆì´ì €ì™€ ëª¨ë¸ì„ ì´ ìƒíƒœì— ì¶”ê°€í•˜ë©´ `train_step()`ê³¼ ê°™ì€ í•¨ìˆ˜ë¡œ ë‹¨ì¼ ì¸ìˆ˜ë§Œ ì „ë‹¬í•˜ë©´ ëœë‹¤ëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤ (ì•„ë˜ ì°¸ì¡°).

ì´ê²ƒì€ ë§¤ìš° ì¼ë°˜ì ì¸ íŒ¨í„´ì´ê¸° ë•Œë¬¸ì— FlaxëŠ” ëŒ€ë¶€ë¶„ì˜ ê¸°ë³¸ ì‚¬ìš© ì‚¬ë¡€ë¥¼ ì œê³µí•˜ëŠ” [flax.training.train_state.TrainState](https://flax.readthedocs.io/en/latest/flax.training.html#train-state) í´ë˜ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ì¶”ì í•  ë°ì´í„°ë¥¼ ë” ì¶”ê°€í•˜ê¸° ìœ„í•´ ì„œë¸Œ í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, ì´ ì˜ˆì œì—ì„œëŠ” ìˆ˜ì • ì—†ì´ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
def create_train_state(rng, learning_rate, momentum):
  """Creates initial `TrainState`."""
  cnn = CNN()
  params = cnn.init(rng, jnp.ones([1, 28, 28, 1]))['params']
  tx = optax.sgd(learning_rate, momentum)
  return train_state.TrainState.create(
      apply_fn=cnn.apply, params=params, tx=tx)
```

### í•™ìŠµ ë‹¨ê³„

ì´ í•¨ìˆ˜ëŠ”:

- [Module.apply](https://flax.readthedocs.io/en/latest/flax.linen.html#flax.linen.Module.apply) í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ì–´ì§„ íŒŒë¼ë¯¸í„°ì™€ ì…ë ¥ ì´ë¯¸ì§€ ë°°ì¹˜ì—ì„œ ì‹ ê²½ë§ì„ í‰ê°€í•©ë‹ˆë‹¤.
- `cross_entropy_loss` ì†ì‹¤ í•¨ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
- [jax.value_and_grad](https://jax.readthedocs.io/en/latest/jax.html#jax.value_and_grad)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì†ì‹¤ í•¨ìˆ˜ì™€ ê·¸ë ˆë””ì–¸íŠ¸ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.
- ì˜µí‹°ë§ˆì´ì €ì— ê·¸ë ˆì´ë””ì–¸íŠ¸ì˜ [pytree](https://jax.readthedocs.io/en/latest/pytrees.html#pytrees-and-jax-functions)ë¥¼ ì ìš©í•˜ì—¬ ëª¨ë¸ì˜ íŒŒë¼ë©”í„°ë¥¼ ì—…ë°ì´íŠ¸ í•©ë‹ˆë‹¤.
- ì•ì„œ ì •ì˜í•œ `compute_metrics` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë©”íŠ¸ë¦­ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

JAXì˜ [@jit](https://jax.readthedocs.io/en/latest/jax.html#jax.jit) ë°ì½”ë ˆì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ì²´ `train_step` í•¨ìˆ˜ë¥¼ ì¶”ì í•˜ê³  ì´ë¥¼ [XLA](https://www.tensorflow.org/xla)ë¥¼ ì‚¬ìš©í•˜ì—¬ í•˜ë“œì›¨ì–´ ê°€ì†ê¸°ì—ì„œ ë” ë¹ ë¥´ê³  íš¨ìœ¨ì ìœ¼ë¡œ ì‹¤í–‰ë˜ëŠ” ìœµí•© ì¥ì¹˜ ì‘ì—…ìœ¼ë¡œ JIT ì»´íŒŒì¼ í•©ë‹ˆë‹¤.

```python
@jax.jit
def train_step(state, batch):
  """Train for a single step."""
  def loss_fn(params):
    logits = CNN().apply({'params': params}, batch['image'])
    loss = cross_entropy_loss(logits=logits, labels=batch['label'])
    return loss, logits
  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)
  (_, logits), grads = grad_fn(state.params)
  state = state.apply_gradients(grads=grads)
  metrics = compute_metrics(logits=logits, labels=batch['label'])
  return state, metrics
```

### í‰ê°€ ë‹¨ê³„

[Module.apply](https://flax.readthedocs.io/en/latest/flax.linen.html#flax.linen.Module.apply)ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ ëª¨ë¸ì„ í‰ê°€í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“­ë‹ˆë‹¤.

```python
@jax.jit
def eval_step(params, batch):
  logits = CNN().apply({'params': params}, batch['image'])
  return compute_metrics(logits=logits, labels=batch['label'])
```

### í•™ìŠµ í•¨ìˆ˜

ë‹¤ìŒê³¼ ê°™ì€ í•™ìŠµ í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤:

- PRNGKeyë¥¼ ë§¤ê°œ ë³€ìˆ˜ë¡œ ì‚¬ìš©í•˜ëŠ” [jax.random.permutation](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.permutation.html)ì„ ì‚¬ìš©í•˜ì—¬ ê° epoch ì „ì— í•™ìŠµ ë°ì´í„°ë¥¼ ì„ìŠµë‹ˆë‹¤.
- ê° ë°°ì¹˜ì— ëŒ€í•´ ìµœì í™” ë‹¨ê³„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
- `jax.device_get`ì„ ì‚¬ìš©í•˜ì—¬ ì¥ì¹˜ì—ì„œ í•™ìŠµ ë©”íŠ¸ë¦­ì„ ê²€ìƒ‰í•˜ê³  epochì˜ ê° ë°°ì¹˜ì—ì„œ í‰ê· ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
- ì—…ë°ì´íŠ¸ëœ íŒŒë¼ë©”í„°ì™€ í•™ìŠµ ì†ì‹¤ ë° ì •í™•ë„ ë©”íŠ¸ë¦­ì´ í¬í•¨ëœ ì˜µí‹°ë§ˆì´ì €ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

```python
def train_epoch(state, train_ds, batch_size, epoch, rng):
  """Train for a single epoch."""
  train_ds_size = len(train_ds['image'])
  steps_per_epoch = train_ds_size // batch_size

  perms = jax.random.permutation(rng, train_ds_size)
  perms = perms[:steps_per_epoch * batch_size]  # skip incomplete batch
  perms = perms.reshape((steps_per_epoch, batch_size))
  batch_metrics = []
  for perm in perms:
    batch = {k: v[perm, ...] for k, v in train_ds.items()}
    state, metrics = train_step(state, batch)
    batch_metrics.append(metrics)

  # compute mean of metrics across each batch in epoch.
  batch_metrics_np = jax.device_get(batch_metrics)
  epoch_metrics_np = {
      k: np.mean([metrics[k] for metrics in batch_metrics_np])
      for k in batch_metrics_np[0]}

  return state, epoch_metrics_np['loss'], epoch_metrics_np['accuracy']
```

### í‰ê°€ í•¨ìˆ˜

ë‹¤ìŒê³¼ ê°™ì€ ëª¨ë¸ í‰ê°€ í•¨ìˆ˜ë¥¼ ë§Œë“­ë‹ˆë‹¤:

- `jax.device_get`ì— ìˆëŠ” ì¥ì¹˜ì—ì„œ í‰ê°€ ë©”íŠ¸ë¦­ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.
- JAX [pytree](https://jax.readthedocs.io/en/latest/pytrees.html#pytrees-and-jax-functions)ì— ì €ì¥ëœ ë©”íŠ¸ë¦­ ë°ì´í„°ë¥¼ ë³µì‚¬í•©ë‹ˆë‹¤.

```python
def eval_model(params, test_ds):
  metrics = eval_step(params, test_ds)
  metrics = jax.device_get(metrics)
  summary = jax.tree_map(lambda x: x.item(), metrics)
  return summary['loss'], summary['accuracy']
```

### í•™ìŠµ ìƒíƒœ ì´ˆê¸°í™”

í•˜ë‚˜ì˜ PRNGKeyë¥¼ ê°€ì ¸ì™€ì„œ ê·¸ê²ƒì„ ë¶„ë¦¬í•˜ì—¬ íŒŒë¼ë©”í„° ì´ˆê¸°í™”ì— ì‚¬ìš©í•  ë‘ ë²ˆì§¸ í‚¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. (ìì„¸í•œ ë‚´ìš©ì€ [PRNG chains](https://flax.readthedocs.io/en/latest/design_notes/linen_design_principles.html#how-are-parameters-represented-and-how-do-we-handle-general-differentiable-algorithms-that-update-stateful-variables)ì™€ [JAX PRNG Design](https://jax.readthedocs.io/en/latest/design_notes/prng.html) ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.)

```python
rng = jax.random.PRNGKey(0)
rng, init_rng = jax.random.split(rng)
```

`create_train_state` í•¨ìˆ˜ëŠ” ëª¨ë¸ íŒŒë¼ë©”í„°ì™€ ì˜µí‹°ë§ˆì´ì € ëª¨ë‘ ì´ˆê¸°í™”í•˜ê³  ë‘˜ ë‹¤ ë°˜í™˜ë˜ëŠ” í›ˆë ¨ ìƒíƒœ ë°ì´í„° í´ë˜ìŠ¤ì— ë„£ìŠµë‹ˆë‹¤.

```python
learning_rate = 0.1
momentum = 0.9

state = create_train_state(init_rng, learning_rate, momentum)
del init_rng  # ì´ˆê¸°í™” ì´í›„ì—ëŠ” ì‚¬ìš©í•˜ì§€ ë§ì•„ì•¼ í•©ë‹ˆë‹¤.
```

### ëª¨ë¸ í•™ìŠµ ë° í‰ê°€

10 epochì´ ì™„ë£Œë˜ë©´ í•™ìŠµ ë°ì´í„°ì…‹ì—ì„œ ëª¨ë¸ ì •í™•ë„ëŠ” ì•½ 93%, í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì—ì„œ ëª¨ë¸ ì •í™•ë„ëŠ” ì•½ 89%ë¥¼ ë‹¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
num_epochs = 10
batch_size = 32

for epoch in range(1, num_epochs + 1):
  # ì…”í”Œë§ ì¤‘ì— ë³„ë„ì˜ PRNG í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ë°ì´í„° ì •ë ¬
  rng, input_rng = jax.random.split(rng)
  # í•™ìŠµ ë°°ì¹˜ì— ëŒ€í•´ ìµœì í™” ë‹¨ê³„ ì‹¤í–‰
  state, train_loss, train_accuracy = train_epoch(state, train_ds, batch_size, epoch, input_rng)
  # ê° í•™ìŠµ epoch í›„ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„¸íŠ¸ì—ì„œ ëª¨ë¸ í‰ê°€
  test_loss, test_accuracy = eval_model(state.params, test_ds)
  print(f"Epoch [{epoch}] - Train loss: {train_loss:.2f}, accuracy: {train_accuracy * 100:.2f}% / " \
            f"Test loss: {test_loss:.2f}, accuracy: {test_accuracy * 100:.2f}%")
```

> Epoch [1] - Train loss: 0.47, accuracy: 82.56% / Test loss: 0.40, accuracy: 84.49% <br>
> Epoch [2] - Train loss: 0.32, accuracy: 87.99% / Test loss: 0.33, accuracy: 87.39% <br>
> Epoch [3] - Train loss: 0.28, accuracy: 89.27% / Test loss: 0.31, accuracy: 88.69% <br>
> Epoch [4] - Train loss: 0.26, accuracy: 90.10% / Test loss: 0.30, accuracy: 89.17% <br>
> Epoch [5] - Train loss: 0.25, accuracy: 90.73% / Test loss: 0.32, accuracy: 88.98% <br>
> Epoch [6] - Train loss: 0.23, accuracy: 91.34% / Test loss: 0.32, accuracy: 89.65% <br>
> Epoch [7] - Train loss: 0.21, accuracy: 91.95% / Test loss: 0.31, accuracy: 89.96% <br>
> Epoch [8] - Train loss: 0.20, accuracy: 92.24% / Test loss: 0.30, accuracy: 90.08% <br>
> Epoch [9] - Train loss: 0.19, accuracy: 92.75% / Test loss: 0.32, accuracy: 89.88% <br>
> Epoch [10] - Train loss: 0.18, accuracy: 93.37% / Test loss: 0.34, accuracy: 89.45% <br>

### ê²°ê³¼ í™•ì¸

í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì˜ ì²« 25ê°œ ì´ë¯¸ì§€ ë°ì´í„°ì˜ ì¶”ë¡  ê²°ê³¼ë¥¼ ì‹œê°í™” í•´ë³´ê² ìŠµë‹ˆë‹¤.

```python
logits = CNN().apply({'params': state.params}, test_ds['image'])
predict_results = logits.argmax(axis=1)  # ë¶€ë™ ì†Œìˆ˜ì  ê°’ì„ ì •ìˆ˜í˜•ìœ¼ë¡œ ë³€í™˜ í•©ë‹ˆë‹¤.

plt.figure(figsize=(10,10))
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(jnp.squeeze(test_ds['image'])[i], cmap=plt.cm.binary)
    plt.xlabel(class_names[predict_results[i]])
plt.show()

```

![Test result](https://github.com/Taehun/taehun.github.io/blob/main/imgs/test_result.jpg?raw=true)

## ê²°ë¡ 

ìœ„ ì˜ˆì œì—ì„œ í™•ì¸ í•  ìˆ˜ ìˆë“¯ì´ Flax ì‚¬ìš©ë²•ì€ ê¸°ì¡´ Tensorflowì™€ PyTorch ì˜ˆì œì—ì„œ ë§ì´ ì ‘í–ˆë˜ ê²ƒë“¤ì´ ì…ë‹ˆë‹¤. PRNGKeyì™€ ê°™ì´ ìƒˆë¡œìš´ ê²ƒë„ ìˆì§€ë§Œ, ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì € ì„¤ì • í•˜ëŠ” ê²ƒì€ ì´ë¯¸ ìµìˆ™í•œ ì½”ë“œ ì…ë‹ˆë‹¤. ì´ ê¸°ì‚¬ ì²« ë¶€ë¶„ì—ë„ ì–¸ê¸‰ í•˜ì˜€ì§€ë§Œ JAXëŠ” ì´ë¯¸ ìƒìš© í”„ë¡œì íŠ¸ì— ì ìš© í•  ìˆ˜ ìˆì„ë§Œí¼ì˜ ì™„ì„±ë„ê°€ ë§ì´ ë†’ì•„ì ¸ì„œ ì§€ê¸ˆë¶€í„°ë¼ë„ ì¤€ë¹„í•´ì„œ JAXì˜ ê°•ì ì„ ì§ì ‘ ê²½í—˜í•´ ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤. Flax/JAX ëª¨ë¸ì˜ ìµœì í™”ë‚˜ ë°°í¬ ê´€ë ¨ ë¶€ë¶„ì€ ì•„ì§ ë¶€ì¡±í•˜ì§€ë§Œ, ë‹¤ë¥¸ ë°°í¬ í¬ë§·ìœ¼ë¡œ ë³€í™˜í•´ì„œ ì‚¬ìš©í•˜ë©´ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¢€ ë” ë§ì€ ë‚´ìš©ì€ ì•„ë˜ JAXì™€ Flax ê³µì‹ ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.

- [Flax ê³µì‹ ë¬¸ì„œ](https://flax.readthedocs.io/en/latest/)
- [JAX ê³µì‹ ë¬¸ì„œ](https://jax.readthedocs.io/en/latest/)
